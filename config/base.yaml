defaults:
  - user: single_user
  - transforms: log_spectrogram
  - model: conformer_ctc  # Conformer + CR-CTC (use cnn_transformer_ctc or tds_conv_crctc for alternatives)
  - optimizer: adamw  # AdamW + decoupled weight decay (use adam for plain Adam)
  - lr_scheduler: reduce_on_plateau  # Adaptive: reduce LR when val metric plateaus (use linear_warmup_cosine_annealing for cosine)
  - decoder: ctc_greedy
  - cluster: local
  - _self_

seed: 1501
batch_size: 32
num_workers: 0  # 0 = main process only (avoids WinError 1455 / paging file on Windows when workers load torch). Use 4 on Linux/Colab for faster loading.
train: True  # Whether to train or only run validation and test
# Conformer training from scratch; set to a .ckpt path to resume
checkpoint: null
# Best checkpoint is selected by this metric (e.g. val/CER); also logged to CSV and TensorBoard
monitor_metric: val/CER
monitor_mode: min

trainer:
  accelerator: gpu
  devices: 1
  num_nodes: 1
  max_epochs: 200
  default_root_dir: ${hydra:runtime.output_dir}
  
  # Real-time logging configuration
  logger:
    - _target_: pytorch_lightning.loggers.TensorBoardLogger
      save_dir: ${hydra:runtime.output_dir}/logs
      name: tensorboard
      version: null
      log_graph: True
      default_hp_metric: True
    # CSV logger: metrics (train/val loss, val/CER, etc.) per epoch for easy inspection and multirun comparison
    - _target_: pytorch_lightning.loggers.CSVLogger
      save_dir: ${hydra:runtime.output_dir}/logs
      name: csv
      version: null
  
  # Frequent logging for real-time updates
  log_every_n_steps: 10  # Log every 10 batches
  val_check_interval: 0.5  # Validate every 0.5 epochs

callbacks:
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step  # Log LR every step (not just epoch)
    
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${hydra:runtime.output_dir}/checkpoints
    monitor: ${monitor_metric}
    mode: ${monitor_mode}
    save_last: True
    verbose: True
  
  - _target_: pytorch_lightning.callbacks.TQDMProgressBar
    refresh_rate: 10  # Update progress bar every 10 batches

dataset:
  root: ${hydra:runtime.cwd}/data

hydra:
  run:
    dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${hydra.run.dir}
    subdir: job${hydra.job.num}_${hydra.job.override_dirname}
  output_subdir: hydra_configs
  job:
    name: emg2qwerty
    config:
      override_dirname:
        exclude_keys:
          - checkpoint
          - cluster
          - trainer.accelerator
